{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51a8f0",
   "metadata": {},
   "source": [
    "# EXTRACT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13260d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile encodings_setup.py\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from scripts.logger import logger\n",
    "from scripts.config import ENV_PATH, CITIES_CONFIG_PATH  # Import paths\n",
    "\n",
    "\n",
    "# loading environment variables (API key)\n",
    "def load_env_api():\n",
    "    \n",
    "    try:\n",
    "        # load envrionment variables from the .env file\n",
    "        load_dotenv(ENV_PATH)\n",
    "\n",
    "        # get the API key from the .env file\n",
    "        api_key = os.getenv(\"API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API_KEY is missing in the environment variables\")\n",
    "        logger.info(\"Successfully loaded API_KEY\")\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading API_KEY: {e}\")\n",
    "        raise\n",
    "\n",
    "# load list of cities to query the API about\n",
    "def load_env_cities():\n",
    "    \n",
    "    try:\n",
    "        # load envrionment variables from the .env file\n",
    "        load_dotenv(ENV_PATH)\n",
    "\n",
    "        # get the list of cities from the .env file\n",
    "        cities_str = os.getenv(\"CITIES\")\n",
    "        if not cities_str:\n",
    "            raise ValueError(\"Cities variable is missing in the env file\")\n",
    "        cities = cities_str.split(\";\")\n",
    "        logger.info(f\"Successfully loaded {len(cities)} cities.\")\n",
    "        return cities\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading cities: {e}\")\n",
    "        raise\n",
    "\n",
    "# getting latitude and longitude encodings for cities (list of cities in config file)\n",
    "## function for getting lat & long encoding of cities\n",
    "def encoding(api_key, cities):\n",
    "    \n",
    "    # Geocoding API endpoint\n",
    "    geocoding_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    # create dataframe to store encodings\n",
    "    encodings = pd.DataFrame(columns=['name', 'latitude', 'longitude'])\n",
    "    \n",
    "    # Loop through the cities and get their lat, lon\n",
    "    for city in cities:\n",
    "        try:\n",
    "            # Send GET request to the OpenWeatherMap Geocoding API\n",
    "            response = requests.get(geocoding_url, params={\n",
    "                'q': city,\n",
    "                'appid': api_key\n",
    "            })\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                lat = data['coord']['lat']\n",
    "                lon = data['coord']['lon']\n",
    "                logger.info(f\"Retrieved coordinate for {city}: ({lat}, {lon})\")\n",
    "                print(f\"City: {city} - Latitude: {lat}, Longitude: {lon}\")\n",
    "                new_row = pd.DataFrame({\"name\": [city], \"latitude\": [lat], \"longitude\": [lon]})\n",
    "                encodings = pd.concat([encodings, new_row], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"Failed to get data for {city}\")\n",
    "                logger.warning(f\"Failed to fetch for data {city}. Status code:{response.status_code}\")\n",
    "        except requets.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request error for {city}: {e}\")\n",
    "            \n",
    "    return encodings;\n",
    "\n",
    "## write the city encodings to the config file\n",
    "def encodings_to_config(encodings):\n",
    "    try:\n",
    "        # Convert the DataFrame to the desired dictionary format\n",
    "        config_data = {\n",
    "            \"cities\": encodings.to_dict(orient=\"records\")  # Convert rows to list of dictionaries\n",
    "        }\n",
    "\n",
    "        # Write the dictionary to a JSON file\n",
    "        with open(CITIES_CONFIG_PATH, 'w') as json_file:\n",
    "            json.dump(config_data, json_file, indent=4)\n",
    "        logger.info(\"Successfully wrote city encodings to cities_config.json\")\n",
    "        print(\"Data has been written to cities_config.json\")\n",
    "    except:\n",
    "        logger.error(f\"Error writing to config file: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # executing the defined functions above \n",
    "    api_key = load_env_api()\n",
    "    cities = load_env_cities()\n",
    "    encodings = encoding(api_key, cities)\n",
    "    encodings_to_config(encodings)\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Pipeline execution failed at encodings: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd26f1c",
   "metadata": {},
   "source": [
    "# API Weather Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile extract.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from scripts.logger import logger\n",
    "from scripts.config import ENV_PATH, CITIES_CONFIG_PATH, RAW_DATA_PATH, RAW_COMPILED_PATH  # Import paths\n",
    "\n",
    "\n",
    "# loading environment variables (API key)\n",
    "def load_env_api():\n",
    "    try:\n",
    "        # load envrionment variables from the .env file\n",
    "        load_dotenv(ENV_PATH)\n",
    "        # get the API key from the .env file\n",
    "        api_key = os.getenv(\"API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API_KEY is missing in the env variables\")\n",
    "        logger.info(\"Successfully loaded API key\")\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading API key: {e}\")\n",
    "        raise\n",
    "\n",
    "# function to execute the API call\n",
    "# api call to get the current weather \n",
    "def get_weather(api_key, city, lat, lon, exclude='minutely,daily,hourly', units='imperial', lang='en'):\n",
    "    # Build the base URL for the OneCall API\n",
    "    api_url = f\"https://api.openweathermap.org/data/3.0/onecall\"\n",
    "    \n",
    "    # Prepare the parameters for the API call\n",
    "    params = {\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'appid': api_key,\n",
    "        'units': units,  # 'imperial' for Fahrenheit, 'metric' for Celsius\n",
    "        'lang': lang      # Language for the response\n",
    "    }\n",
    "    \n",
    "    # Add the 'exclude' parameter if it's provided\n",
    "    if exclude:\n",
    "        params['exclude'] = exclude\n",
    "        \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(api_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        # Check if the request was successful\n",
    "        data = response.json()\n",
    "        # Print or process the data\n",
    "        data['City']=city\n",
    "        logger.info(f\"Successfully fetched weather data for {city}\")\n",
    "        print(data)\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        logger.error(f\"Error fetching data from {api_url}: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to decode JSON response for {city}: {e}\")\n",
    "    return None\n",
    "    \n",
    "\n",
    "# function to run the API call based upon cities in config file\n",
    "def city_weather_data_extraction():\n",
    "    try:\n",
    "        # Step 1: Load the config file\n",
    "        with open(CITIES_CONFIG_PATH, 'r') as f:\n",
    "            config_data = json.load(f)\n",
    "    except (IOError, json.JSONDecodeError) as e:\n",
    "        logger.error(f\"Error loading cities config file: {e}\")\n",
    "        raise\n",
    "\n",
    "    weather_data = []\n",
    "\n",
    "    # Step 2: Loop through each city and use the data for API requests\n",
    "    for city in config_data['cities']:\n",
    "        api_key = load_env_api()\n",
    "        latitude = city['latitude']\n",
    "        longitude = city['longitude']\n",
    "        city_name = city['name']\n",
    "        \n",
    "        if not all([city_name,latitude,longitude]):\n",
    "            logger.warning(f\"skipping city due to missing data: {city}\")\n",
    "            continue\n",
    "\n",
    "        weather_data.append(get_weather(api_key, city_name, latitude, longitude))\n",
    "        \n",
    "    return weather_data\n",
    "\n",
    "# writing the extracted data to the raw_weather_data.json file\n",
    "def write_raw_data(weather_data):\n",
    "\n",
    "    try:\n",
    "        # Check if the file exists to decide whether to append or create new\n",
    "        if os.path.exists(RAW_DATA_PATH):\n",
    "            # If the file exists, load the existing data, then append new data\n",
    "            with open(RAW_DATA_PATH, 'r') as json_file:\n",
    "                existing_data = json.load(json_file)\n",
    "                existing_data.extend(weather_data)\n",
    "\n",
    "            # Append to the file\n",
    "            with open(RAW_DATA_PATH, 'w') as json_file:\n",
    "                json.dump(existing_data, json_file, indent=4)\n",
    "        else:\n",
    "            # If the file doesn't exist, create it and write the new data\n",
    "            with open(RAW_DATA_PATH, 'w') as json_file:\n",
    "                json.dump(weather_data, json_file, indent=4)\n",
    "\n",
    "        print(f\"Data saved to {RAW_DATA_PATH}\")\n",
    "        logger.info(f\"Weather data successfully saved to {RAW_DATA_PATH}\")\n",
    "    except (IOError, json.JSONDecodeError, ValueError) as e:\n",
    "        logger.error(f\"Error writing to {RAW_FILE_PATH}: {e}\")\n",
    "        raise\n",
    "\n",
    "    \n",
    "# writing the extracted data to the raw_compiled_data.json file\n",
    "def write_compiled_raw_data(weather_data):\n",
    "    \n",
    "    try:\n",
    "        # Check if the file exists to decide whether to append or create new\n",
    "        if os.path.exists(RAW_COMPILED_PATH):\n",
    "            # If the file exists, load the existing data, then append new data\n",
    "            with open(RAW_COMPILED_PATH, 'r') as json_file:\n",
    "                existing_data = json.load(json_file)\n",
    "                existing_data.extend(weather_data)\n",
    "\n",
    "            # Append to the file\n",
    "            with open(RAW_COMPILED_PATH, 'w') as json_file:\n",
    "                json.dump(existing_data, json_file, indent=4)\n",
    "        else:\n",
    "            # If the file doesn't exist, create it and write the new data\n",
    "            with open(RAW_COMPILED_PATH, 'w') as json_file:\n",
    "                json.dump(weather_data, json_file, indent=4)\n",
    "\n",
    "        print(f\"Data saved to {RAW_COMPILED_PATH}\")\n",
    "        logger.info(f\"Weather data successfully saved to {RAW_COMPILED_PATH}\")\n",
    "    except (IOError, json.JSONDecodeError, ValueError) as e:\n",
    "        logger.error(f\"Error writing to {RAW_COMPILED_PATH}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "try:\n",
    "    # executing the api calls & writing to the file functions\n",
    "    weather_data = city_weather_data_extraction()\n",
    "    write_raw_data(weather_data)\n",
    "    write_compiled_raw_data(weather_data)\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Pipeline extraction failed at extract.py: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31a64c",
   "metadata": {},
   "source": [
    "# Transform:\n",
    "clean the weather data \n",
    "write it to a clean_data and db_ready csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51119c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transform.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from scripts.logger import logger\n",
    "from scripts.config import RAW_DATA_PATH, CLEAN_DATA_PATH  # Import paths\n",
    "\n",
    "\n",
    "# reading the raw data from the raw_weather_data.json file\n",
    "def read_raw_data():\n",
    "    \n",
    "    try:\n",
    "        with open(RAW_DATA_PATH, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        logger.info(f\"Successfully loaded raw weather data from {RAW_DATA_PATH}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: the file '{RAW_DATA_PATH}' does not exist\")\n",
    "        logger.error(f\"Error: the file {RAW_DATA_PATH} does not exist\")\n",
    "        return None\n",
    "    except json.jSONDecodeError as e:\n",
    "        print(f\"Error: failed to decode JSON frim the file '{RAW_DATA_PATH}'\")\n",
    "        logger.error(f\"Error: failed to decode JSON from the file {RAW_DATA_PATH}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error while reading raw data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to convert Unix timestamp to local time\n",
    "def convert_to_local_time(timestamp, offset):\n",
    "    try:\n",
    "        utc_time = datetime.utcfromtimestamp(timestamp)\n",
    "        return utc_time + timedelta(seconds=offset)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting timestamp {timestamp} with offset {offset}: {e}\")\n",
    "        return None\n",
    "\n",
    "# processing & cleaning the weather data & storing in a dataframe\n",
    "def transform_data(weather_data):\n",
    "    if not weather_data:\n",
    "        logger.error(\"No data provided for transformation\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize an empty list to store records\n",
    "    data = []\n",
    "\n",
    "    try:\n",
    "        # Process each record\n",
    "        for record in weather_data:\n",
    "            latitude, longitude = record[\"lat\"], record[\"lon\"]\n",
    "            timezone = record[\"timezone\"]\n",
    "            timezone_offset = record[\"timezone_offset\"]\n",
    "            city = record['City']\n",
    "            \n",
    "            if latitude is None or longitude is None:\n",
    "                logger.warning(\"skipping record due to missing latitude/longitude\")\n",
    "                continue\n",
    "\n",
    "            # Convert timestamps\n",
    "            current_time = convert_to_local_time(record[\"current\"][\"dt\"], timezone_offset)\n",
    "            sunrise = convert_to_local_time(record[\"current\"][\"sunrise\"], timezone_offset)\n",
    "            sunset = convert_to_local_time(record[\"current\"][\"sunset\"], timezone_offset)\n",
    "\n",
    "            # Extract weather details\n",
    "            temp = record[\"current\"][\"temp\"]\n",
    "            feels_like = record[\"current\"][\"feels_like\"]\n",
    "            pressure = record[\"current\"][\"pressure\"]\n",
    "            humidity = record[\"current\"][\"humidity\"]\n",
    "            dew_point = record[\"current\"][\"dew_point\"]\n",
    "            uvi = record[\"current\"][\"uvi\"]\n",
    "            clouds = record[\"current\"][\"clouds\"]\n",
    "            visibility = record[\"current\"][\"visibility\"]\n",
    "            wind_speed = record[\"current\"][\"wind_speed\"]\n",
    "            wind_deg = record[\"current\"][\"wind_deg\"]\n",
    "            wind_gust = record[\"current\"].get(\"wind_gust\", 0)\n",
    "            weather = record[\"current\"][\"weather\"][0]\n",
    "            weather_id = weather[\"id\"]\n",
    "            weather_main = weather[\"main\"]\n",
    "            weather_description = weather[\"description\"]\n",
    "\n",
    "            # Handle alerts (if any)\n",
    "            alerts = record.get(\"alerts\", [])\n",
    "            alert_messages = \"; \".join([alert[\"event\"] + \": \" + alert[\"description\"] for alert in alerts])\n",
    "\n",
    "            # Add the record to the data list\n",
    "            data.append({\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude,\n",
    "                \"timezone\": timezone,\n",
    "                \"timezone_offset\": timezone_offset,\n",
    "                \"city\": city,\n",
    "                \"current_time\": current_time,\n",
    "                \"sunrise\": sunrise,\n",
    "                \"sunset\": sunset,\n",
    "                \"temp_F\": temp,\n",
    "                \"feels_like_F\": feels_like,\n",
    "                \"humidity\": humidity,\n",
    "                \"dew_point\": dew_point,\n",
    "                \"uvi\": uvi,\n",
    "                \"clouds\": clouds,\n",
    "                \"visibility\": visibility,\n",
    "                \"wind_speed_mph\": wind_speed,\n",
    "                \"wind_deg\": wind_deg,\n",
    "                \"wind_gust_mph\": wind_gust,\n",
    "                \"weather_id\": weather_id,\n",
    "                \"weather_main\": weather_main,\n",
    "                \"weather_description\": weather_description,\n",
    "                \"alerts\": alert_messages\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the data list\n",
    "        df = pd.DataFrame(data)\n",
    "        logger.info(\"Successfully transformed weather data into DataFrame\")\n",
    "        return df\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Missing expected key in weather data: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.erorr(f\"Unexpected error during transformation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# writing the cleaned data to the compiled clean_weather_data.csv file\n",
    "def write_to_cleaned_data(df):\n",
    "    if df is None or df.empty:\n",
    "        logger.error(\"No data available to write to CSV\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Check if the file exists to decide whether to append or create new\n",
    "        if os.path.exists(CLEAN_DATA_PATH):\n",
    "            # If the file exists, load the existing data, then append new data\n",
    "            existing_data = pd.read_csv(CLEAN_DATA_PATH)\n",
    "            updated_data = pd.concat([existing_data, df], ignore_index=True)\n",
    "\n",
    "            # Append to the file\n",
    "            updated_data.to_csv(CLEAN_DATA_PATH, index=False)\n",
    "        else:\n",
    "            # If the file doesn't exist, create it and write the new data\n",
    "            df.to_csv(CLEAN_DATA_PATH, index=False)\n",
    "\n",
    "        print(f\"Data saved to {CLEAN_DATA_PATH}\")\n",
    "        logger.info(f\"Data successfully saved to {CLEAN_DATA_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing data to CSV: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    # execution of the transformation functions\n",
    "    data = read_raw_data()\n",
    "    df = transform_data(data)\n",
    "    write_to_cleaned_data(df)\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Pipeline execution failed at transfrom.py script: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c48b47",
   "metadata": {},
   "source": [
    "# Load:\n",
    "write the cleaned data to a postgresql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b8188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from scripts.logger import logger\n",
    "from scripts.config import ENV_PATH, RAW_DATA_PATH, CLEAN_DATA_PATH  # Import paths\n",
    "\n",
    "\n",
    "# function to read the clean_data file and return a dataframe\n",
    "def read_clean_data():\n",
    "    try:\n",
    "        df_db = pd.read_csv(CLEAN_DATA_PATH)\n",
    "        logger.info(f\"Successfully loaded clean data from {CLEAN_DATA_PATH}\")\n",
    "        return df_db\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Error: the file {CLEAN_DATA_PATH} does not exist\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(f\"Error: the file {CLEAN_DATA_PATH} is empty\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error while reading clean data: {e}\")\n",
    "    return None\n",
    "\n",
    "# function to load env variables & establish database connection\n",
    "def env_db_connection():\n",
    "    ## load environment variables and establish database connection\n",
    "\n",
    "    load_dotenv(ENV_PATH)\n",
    "\n",
    "\n",
    "    try:\n",
    "           ## get local database credentials\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=os.getenv(\"DB_NAME\"),\n",
    "            user=os.getenv(\"DB_USER\"),\n",
    "            password=os.getenv(\"DB_PASSWORD\"),\n",
    "            host=os.getenv(\"DB_HOST\"),\n",
    "            port=os.getenv(\"DB_PORT\")\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Connected to PostgreSQL database\")\n",
    "        print(\"connected to postgresql on local host\")\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"connection error: {e}\")\n",
    "        logger.error(f\"Database connection error: {e}\")\n",
    "    return None\n",
    "        \n",
    "\n",
    "# function to insert weather data into the location database table\n",
    "# Insert or get Location_ID\n",
    "def get_or_insert_location(cursor, lat, lon, city, timezone, tz_offset):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"SELECT Location_ID FROM Locations WHERE Lat=%s AND Long=%s;\",\n",
    "            (lat, lon)\n",
    "        )\n",
    "        location = cursor.fetchone()\n",
    "        if location:\n",
    "            print(\"location is already stored in the database\")\n",
    "            return location[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO Locations (Lat, Long, City, Timezone, Timezone_offset) VALUES (%s, %s, %s, %s, %s) RETURNING Location_ID;\",\n",
    "                (lat, lon, city, timezone, tz_offset)\n",
    "            )\n",
    "            return cursor.fetchone()[0]\n",
    "    except Error as e:\n",
    "        logger.error(f\"Error inserting location: {e}\")\n",
    "    return None\n",
    "\n",
    "# function to insert weather data into the weather database table\n",
    "# Insert or get Weather_ID\n",
    "def get_or_insert_weather(cursor, weather_id, main, description):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"SELECT Weather_ID FROM Weather where Weather_ID=%s;\",\n",
    "            (weather_id,)\n",
    "        )\n",
    "        weather = cursor.fetchone()\n",
    "        if weather:\n",
    "            print(\"weather is already stored in the database\")\n",
    "            return weather[0]\n",
    "        else:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO Weather (Weather_ID, Main, Description) VALUES (%s, %s, %s) RETURNING Weather_ID;\",\n",
    "                (weather_id, main, description)\n",
    "            )\n",
    "            return cursor.fetchone()[0]\n",
    "    except Error as e:\n",
    "        logger.error(f\"Error inserting weather data: {e}\")\n",
    "    return None\n",
    "\n",
    "# function to insert weather data into the record database table\n",
    "# Insert Record\n",
    "def insert_record(cursor, location_id, weather_id, row):\n",
    "    try:\n",
    "        if not isinstance(location_id, int) or not isinstance(weather_id, int):\n",
    "            logger.error(\"Invalid location_id or weather_id: skipping weather insertion\")\n",
    "            return None\n",
    "        \n",
    "        cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO Records (Location_ID, Weather_ID, Local_time, Sunrise, Sunset, Temp_F, Feels_like_F, \n",
    "                            Humidity, Dew_Point, UVI, Clouds, Visibility, Wind_speed_mph, Wind_deg, Wind_gust_mph)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING Record_ID;\n",
    "        \"\"\",\n",
    "        (location_id, weather_id, row['current_time'], row['sunrise'], row['sunset'], row['temp_F'],\n",
    "            row['feels_like_F'], row['humidity'], row['dew_point'], row['uvi'], row['clouds'], row['visibility'], \n",
    "            row['wind_speed_mph'], row['wind_deg'], row['wind_gust_mph'])\n",
    "        )\n",
    "        print(\"record has been inserted into the table\")\n",
    "        logger.info(\"Record has been inserted into the table\")\n",
    "        return cursor.fetchone()[0]\n",
    "    except Error as e:\n",
    "        logger.error(f\"Error inserting record: {e}\")\n",
    "    return None\n",
    "\n",
    "# function to insert weather data into the alert database table\n",
    "# Insert Alert\n",
    "def insert_alert(cursor, record_id, alert_description):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO Alerts (Record_ID, Description) VALUES (%s, %s);\",\n",
    "            (record_id, alert_description)\n",
    "        )\n",
    "        print(\"alert has been inserted into the table\")\n",
    "        logger.info(\"alert has been inserted into the table\")\n",
    "    except Error as e:\n",
    "        logger.erorr(f\"Error inserting alert: {e}\")\n",
    "\n",
    "# function to delete the clean_data csv file once the data has been uploaded to avoid duplication of data loading/records\n",
    "def delete_clean_data():\n",
    "    try:\n",
    "        if os.path.exists(CLEAN_DATA_PATH):\n",
    "            os.remove(CLEAN_DATA_PATH)\n",
    "            print(\"File deleted successfully\")\n",
    "            logger.info(f\"File {CLEAN_DATA_PATH} deleted successfully\")\n",
    "        else:\n",
    "            print(\"File does not exist\")\n",
    "            logger.warning(f\"File {CLEAN_DATA_PATH} does not exist\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting file {CLEAN_DATA_PATH}: {e}\")\n",
    "        \n",
    "# function to delete the raw_data csv file once the data has been uploaded to avoid duplication of data loading/records\n",
    "def delete_raw_data():\n",
    "    try:\n",
    "        if os.path.exists(RAW_DATA_PATH):\n",
    "            os.remove(RAW_DATA_PATH)\n",
    "            print(\"File deleted successfully\")\n",
    "            logger.info(f\"File {RAW_DATA_PATH} deleted successfully\")\n",
    "        else:\n",
    "            print(\"File does not exist\")\n",
    "            logger.info(f\"File {RAW_DATA_PATH} does not exisit\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting file {RAW_DATA_PATH}: {e}\")\n",
    "\n",
    "\n",
    "# execution of functions above and the load process\n",
    "\n",
    "df = read_clean_data()\n",
    "if df is not None:\n",
    "    conn = env_db_connection()\n",
    "    if conn:\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                for _, row in df.iterrows():\n",
    "                    location_id = get_or_insert_location(cursor, row['latitude'], row['longitude'], row['city'], row['timezone'], row['timezone_offset'])\n",
    "                    weather_id = get_or_insert_weather(cursor, row['weather_id'], row['weather_main'], row['weather_description'])\n",
    "                    record_id = insert_record(cursor, location_id, weather_id, row)\n",
    "\n",
    "                    if pd.notna(row['alerts']) and row['alerts'].strip():\n",
    "                        insert_alert(cursor, record_id, row['alerts'])\n",
    "                conn.commit()\n",
    "                logger.info(\"Data successfully inserted into database\")\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"Error during data insertion {e}\")\n",
    "            success = False\n",
    "        finally:\n",
    "            conn.close()\n",
    "            logger.info(\"database connection closed\")\n",
    "            \n",
    "        if success:\n",
    "            delete_clean_data()\n",
    "            delete_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b28241",
   "metadata": {},
   "source": [
    "# Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "## using this for maintaining the directories\n",
    "import os\n",
    "\n",
    "# Define the base directory explicitly\n",
    "BASE_DIR = r\"C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\"\n",
    "\n",
    "# Log file path\n",
    "LOG_Path = os.path.join(BASE_DIR,\"logs\",\"pipeline.log\")\n",
    "\n",
    "# Common file paths\n",
    "CITIES_CONFIG_PATH = os.path.join(BASE_DIR,\"cities_config.json\")\n",
    "RAW_COMPILED_PATH = os.path.join(BASE_DIR, \"data\", \"raw_compiled_data.json\")\n",
    "RAW_DATA_PATH = os.path.join(BASE_DIR, \"data\", \"raw_weather_data.json\")\n",
    "CLEAN_DATA_PATH = os.path.join(BASE_DIR, \"data\", \"clean_weather_data.csv\")\n",
    "ENV_PATH = os.path.join(BASE_DIR, \".env\")  # If you store your env file here\n",
    "LOG_PATH = os.path.join(BASE_DIR, \"logs\", \"pipeline.log\")  # Example log file path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a527192",
   "metadata": {},
   "source": [
    "# Main Pipeline Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ead6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile etl_pipeline.py\n",
    "\n",
    "import pandas as pd\n",
    "from scripts.logger import logger\n",
    "from scripts.encodings_setup import load_env_api, load_env_cities, encoding, encodings_to_config\n",
    "from scripts.extract import city_weather_data_extraction, write_raw_data, write_compiled_raw_data\n",
    "from scripts.transform import read_raw_data, transform_data, write_to_cleaned_data\n",
    "from scripts.load import read_clean_data, env_db_connection, get_or_insert_location, get_or_insert_weather, insert_record, insert_alert, delete_clean_data, delete_raw_data\n",
    "\n",
    "def run_pipeline():\n",
    "    try:\n",
    "        print(\"Starting the ETL process \\n\")\n",
    "        logger.info(\"Starting the ETL process\")\n",
    "\n",
    "        print(\"Encoding cities \\n\")\n",
    "        logger.info(\"Encoding cities\")\n",
    "        #Encoding\n",
    "        api_key = load_env_api()\n",
    "        cities = load_env_cities()\n",
    "        encodings = encoding(api_key, cities)\n",
    "        encodings_to_config(encodings)\n",
    "        print(\"Encoding Complete \\n\")\n",
    "        logger.info(\"Encoding complete\")\n",
    "\n",
    "        #Extract Data\n",
    "        print(\"Extracting Data \\n\")\n",
    "        logger.info(\"Extracting data\")\n",
    "        weather_data = city_weather_data_extraction()\n",
    "        if weather_data is None:\n",
    "            logger.error(\"Weather data extraction failed\")\n",
    "            return\n",
    "        write_raw_data(weather_data)\n",
    "        print(\"Data Extraction Complete\")\n",
    "        logger.info(\"Data extraction complete\")\n",
    "\n",
    "\n",
    "        #Transform Data\n",
    "        print(\"Transforming Data \\n\") \n",
    "        logger.info(\"Transforming Data\")\n",
    "        data = read_raw_data()\n",
    "        df = transform_data(data)\n",
    "        if df is None:\n",
    "            logger.error(\"Data transformation failed\")\n",
    "            return\n",
    "        write_to_cleaned_data(df)\n",
    "        print(\"Transforming Data Complete \\n\")\n",
    "        logger.info(\"Data Transformation Complete\")\n",
    "\n",
    "        #Load Data\n",
    "        print(\"Loading Data \\n\")\n",
    "        logger.info(\"Loading Data\")\n",
    "        df = read_clean_data()\n",
    "        if df is None or df.empty:\n",
    "            logger.error(\"No clean data available for loading\")\n",
    "            return\n",
    "        \n",
    "        conn = env_db_connection()\n",
    "        if conn is None:\n",
    "            logger.error(\"Database connection failed\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                for _, row in df.iterrows():\n",
    "                    location_id = get_or_insert_location(cursor, row['latitude'], row['longitude'], row['city'], row['timezone'], row['timezone_offset'])\n",
    "                    weather_id = get_or_insert_weather(cursor, row['weather_id'], row['weather_main'], row['weather_description'])\n",
    "                    record_id = insert_record(cursor, location_id, weather_id, row)\n",
    "\n",
    "                    if pd.notna(row['alerts']) and row['alerts'].strip():\n",
    "                        insert_alert(cursor, record_id, row['alerts'])\n",
    "                conn.commit()\n",
    "            logger.info(\"data inserted successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while inserting data: {e}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "            print(\"data inserted successfully & connection closed\")\n",
    "            logger.info(\"Database connection closed\")\n",
    "            \n",
    "        delete_clean_data()\n",
    "        delete_raw_data()\n",
    "        print(\"Loading Data Complete \\n\")\n",
    "        logger.info(\"Loading Data Complete\")\n",
    "\n",
    "        print(\"ETL process completed successfully\")\n",
    "        logger.info(\"ETL process completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Pipeline execution failed: {e}\", exc_info=True)\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1db77",
   "metadata": {},
   "source": [
    "# Testing scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eca57424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Denver,CO,USA - Latitude: 39.7392, Longitude: -104.9847\n",
      "City: Austin,TX,USA - Latitude: 30.2711, Longitude: -97.7437\n",
      "City: Stuttgart,DE - Latitude: 48.7823, Longitude: 9.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\scripts\\encodings_setup.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  encodings = pd.concat([encodings, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to cities_config.json\n"
     ]
    }
   ],
   "source": [
    "%run scripts/encodings_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22885826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat': 39.7392, 'lon': -104.9847, 'timezone': 'America/Denver', 'timezone_offset': -25200, 'current': {'dt': 1740755843, 'sunrise': 1740749658, 'sunset': 1740790251, 'temp': 45.9, 'feels_like': 44.13, 'pressure': 1017, 'humidity': 30, 'dew_point': 17.98, 'uvi': 0.67, 'clouds': 0, 'visibility': 10000, 'wind_speed': 4, 'wind_deg': 69, 'wind_gust': 8.99, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}]}, 'City': 'Denver,CO,USA'}\n",
      "{'lat': 30.2711, 'lon': -97.7437, 'timezone': 'America/Chicago', 'timezone_offset': -21600, 'current': {'dt': 1740755843, 'sunrise': 1740747483, 'sunset': 1740788951, 'temp': 58.75, 'feels_like': 56.77, 'pressure': 1021, 'humidity': 52, 'dew_point': 41.2, 'uvi': 1.65, 'clouds': 20, 'visibility': 10000, 'wind_speed': 1.99, 'wind_deg': 256, 'weather': [{'id': 801, 'main': 'Clouds', 'description': 'few clouds', 'icon': '02d'}]}, 'City': 'Austin,TX,USA'}\n",
      "{'lat': 48.7823, 'lon': 9.177, 'timezone': 'Europe/Berlin', 'timezone_offset': 3600, 'current': {'dt': 1740755843, 'sunrise': 1740722840, 'sunset': 1740762279, 'temp': 41.25, 'feels_like': 35.53, 'pressure': 1024, 'humidity': 80, 'dew_point': 35.56, 'uvi': 0.14, 'clouds': 0, 'visibility': 10000, 'wind_speed': 9.22, 'wind_deg': 290, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}]}, 'alerts': [{'sender_name': 'Deutscher Wetterdienst', 'event': 'frost', 'start': 1740769200, 'end': 1740816000, 'description': 'There is a risk of frost (level 1 of 2).\\nMinimum temperature: 0 - -4 °C', 'tags': ['Extreme low temperature']}, {'sender_name': 'Deutscher Wetterdienst', 'event': 'icy surfaces', 'start': 1740769200, 'end': 1740816000, 'description': 'There is a risk of icy surfaces (level 1 of 4).', 'tags': ['Snow/Ice']}], 'City': 'Stuttgart,DE'}\n",
      "Data saved to C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\data\\raw_weather_data.json\n",
      "Data saved to C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\data\\raw_compiled_data.json\n"
     ]
    }
   ],
   "source": [
    "%run scripts/extract.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b73beabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\data\\clean_weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "%run scripts/transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e07f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to postgresql on local host\n",
      "location is already stored in the database\n",
      "weather is already stored in the database\n",
      "record has been inserted into the table\n",
      "location is already stored in the database\n",
      "weather is already stored in the database\n",
      "record has been inserted into the table\n",
      "location is already stored in the database\n",
      "weather is already stored in the database\n",
      "record has been inserted into the table\n",
      "alert has been inserted into the table\n",
      "File deleted successfully\n",
      "File deleted successfully\n"
     ]
    }
   ],
   "source": [
    "%run scripts/load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bd97eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: Denver,CO,USA - Latitude: 39.7392, Longitude: -104.9847\n",
      "City: Austin,TX,USA - Latitude: 30.2711, Longitude: -97.7437\n",
      "City: Stuttgart,DE - Latitude: 48.7823, Longitude: 9.177\n",
      "Data has been written to cities_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\scripts\\encodings_setup.py:74: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  encodings = pd.concat([encodings, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lat': 39.7392, 'lon': -104.9847, 'timezone': 'America/Denver', 'timezone_offset': -25200, 'current': {'dt': 1740756077, 'sunrise': 1740749658, 'sunset': 1740790251, 'temp': 46.78, 'feels_like': 45.14, 'pressure': 1017, 'humidity': 29, 'dew_point': 17.98, 'uvi': 0.67, 'clouds': 0, 'visibility': 10000, 'wind_speed': 4, 'wind_deg': 208, 'wind_gust': 8.01, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}]}, 'City': 'Denver,CO,USA'}\n",
      "{'lat': 30.2711, 'lon': -97.7437, 'timezone': 'America/Chicago', 'timezone_offset': -21600, 'current': {'dt': 1740756077, 'sunrise': 1740747483, 'sunset': 1740788951, 'temp': 59, 'feels_like': 57.04, 'pressure': 1021, 'humidity': 52, 'dew_point': 41.43, 'uvi': 1.65, 'clouds': 20, 'visibility': 10000, 'wind_speed': 1.99, 'wind_deg': 336, 'wind_gust': 1.99, 'weather': [{'id': 801, 'main': 'Clouds', 'description': 'few clouds', 'icon': '02d'}]}, 'City': 'Austin,TX,USA'}\n",
      "{'lat': 48.7823, 'lon': 9.177, 'timezone': 'Europe/Berlin', 'timezone_offset': 3600, 'current': {'dt': 1740756077, 'sunrise': 1740722840, 'sunset': 1740762279, 'temp': 41.25, 'feels_like': 35.53, 'pressure': 1024, 'humidity': 81, 'dew_point': 35.87, 'uvi': 0.14, 'clouds': 0, 'visibility': 10000, 'wind_speed': 9.22, 'wind_deg': 290, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}]}, 'alerts': [{'sender_name': 'Deutscher Wetterdienst', 'event': 'frost', 'start': 1740769200, 'end': 1740816000, 'description': 'There is a risk of frost (level 1 of 2).\\nMinimum temperature: 0 - -4 °C', 'tags': ['Extreme low temperature']}, {'sender_name': 'Deutscher Wetterdienst', 'event': 'icy surfaces', 'start': 1740769200, 'end': 1740816000, 'description': 'There is a risk of icy surfaces (level 1 of 4).', 'tags': ['Snow/Ice']}], 'City': 'Stuttgart,DE'}\n",
      "Data saved to C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\data\\raw_weather_data.json\n",
      "Data saved to C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\data\\raw_compiled_data.json\n",
      "Data saved to C:\\Users\\matth\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\data\\clean_weather_data.csv\n"
     ]
    },
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 185 (load.py, line 186)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m~\\OneDrive\\Desktop\\DataEngineering\\WeatherETLPipeline\\scripts\\load.py:186\u001b[1;36m\u001b[0m\n\u001b[1;33m    delete_clean_data()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 185\n"
     ]
    }
   ],
   "source": [
    "%run scripts/etl_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ae502",
   "metadata": {},
   "source": [
    "#  Log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ac3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile logger.py\n",
    "\n",
    "import logging\n",
    "from scripts.config import LOG_PATH\n",
    "\n",
    "# clear existing handlers to prevent duplicates\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Configure Logging w/ the correct log file path\n",
    "logging.basicConfig(\n",
    "    filename=LOG_PATH,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger.info(\"Logger initialized successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
